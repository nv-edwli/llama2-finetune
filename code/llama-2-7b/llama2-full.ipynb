{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c62076",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /project/data && git lfs clone https://huggingface.co/datasets/databricks/databricks-dolly-15k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b91b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/NeMo-Megatron-Launcher/launcher_scripts/nemo_launcher/collections/dataprep_scripts/dolly_dataprep/preprocess.py --input /project/data/databricks-dolly-15k/databricks-dolly-15k.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e77fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 /project/data/databricks-dolly-15k/databricks-dolly-15k-output.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ba089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "input_file = \"/project/data/databricks-dolly-15k/databricks-dolly-15k-output.jsonl\"\n",
    "training_output_file = \"/project/data/databricks-dolly-15k/training.jsonl\"\n",
    "validation_output_file = \"/project/data/databricks-dolly-15k/validation.jsonl\"\n",
    "test_output_file = \"/project/data/databricks-dolly-15k/test.jsonl\"\n",
    "\n",
    "# Specify the proportion of data for training and validation\n",
    "train_proportion = 0.80\n",
    "validation_proportion = 0.15\n",
    "test_proportion = 0.05\n",
    "\n",
    "# Read the JSONL file and shuffle the JSON objects\n",
    "with open(input_file, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    random.shuffle(lines)\n",
    "\n",
    "# Calculate split indices\n",
    "total_lines = len(lines)\n",
    "train_index = int(total_lines * train_proportion)\n",
    "val_index = int(total_lines * validation_proportion)\n",
    "\n",
    "# Distribute JSON objects into training and validation sets\n",
    "train_data = lines[:train_index]\n",
    "validation_data = lines[train_index:train_index+val_index]\n",
    "test_data = lines[train_index+val_index:]\n",
    "\n",
    "# Write JSON objects to training file\n",
    "with open(training_output_file, \"w\") as f:\n",
    "    for line in train_data:\n",
    "        f.write(line.strip() + \"\\n\")\n",
    "\n",
    "# Write JSON objects to validation file\n",
    "with open(validation_output_file, \"w\") as f:\n",
    "    for line in validation_data:\n",
    "        f.write(line.strip() + \"\\n\")\n",
    "\n",
    "# Write JSON objects to training file\n",
    "with open(test_output_file, \"w\") as f:\n",
    "    for line in test_data:\n",
    "        f.write(line.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872c86bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"/project/models/llama2-7b.nemo\"\n",
    "TRAIN=\"/project/data/databricks-dolly-15k/train.jsonl\"\n",
    "VALID=\"/project/data/databricks-dolly-15k/validation.jsonl\"\n",
    "TEST=\"/project/data/databricks-dolly-15k/test.jsonl\"\n",
    "VALID_NAMES=\"databricks-dolly-15k\"\n",
    "CONCAT_SAMPLING_PROBS=\"[1]\"\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d4500",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load(\"/opt/NeMo/examples/nlp/language_modeling/tuning/conf/megatron_gpt_peft_tuning_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb3d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.precision=\"bf16\" \n",
    "trainer.devices=8 \n",
    "trainer.num_nodes=1 \n",
    "trainer.val_check_interval=0.1 \n",
    "trainer.max_steps=50 \n",
    "model.restore_from_path=MODEL\n",
    "model.micro_batch_size=1 \n",
    "model.global_batch_size=128 \n",
    "model.tensor_model_parallel_size=TP_SIZE\n",
    "model.pipeline_model_parallel_size=PP_SIZE\n",
    "model.megatron_amp_O2=True \n",
    "model.sequence_parallel=True \n",
    "model.activations_checkpoint_granularity=\"selective\" \n",
    "model.activations_checkpoint_method=\"uniform\"\n",
    "model.optim.name=\"distributed_fused_adam\" \n",
    "model.optim.lr=5e-6 \n",
    "model.answer_only_loss=True\n",
    "model.data.train_ds.file_names=TRAIN_DS\n",
    "model.data.validation_ds.file_names=VALID_DS\n",
    "model.data.test_ds.file_names=TEST_DS\n",
    "model.data.train_ds.concat_sampling_probabilities=CONCAT_SAMPLING_PROBS\n",
    "model.data.train_ds.max_seq_length=2048 \n",
    "model.data.validation_ds.max_seq_length=2048 \n",
    "model.data.train_ds.micro_batch_size=1 \n",
    "model.data.train_ds.global_batch_size=128 \n",
    "model.data.validation_ds.micro_batch_size=1 \n",
    "model.data.validation_ds.global_batch_size=128 \n",
    "model.data.test_ds.micro_batch_size=1 \n",
    "model.data.test_ds.global_batch_size=256 \n",
    "model.data.train_ds.num_workers=0 \n",
    "model.data.validation_ds.num_workers=0\n",
    "model.data.test_ds.num_workers=0 \n",
    "model.data.validation_ds.metric.name=\"loss\" \n",
    "model.data.test_ds.metric.name=\"loss\" \n",
    "exp_manager.create_wandb_logger=False \n",
    "exp_manager.explicit_log_dir=\"/project/code/llama-2-7b/results\"\n",
    "exp_manager.resume_if_exists=True \n",
    "exp_manager.resume_ignore_no_checkpoint=True \n",
    "exp_manager.create_checkpoint_callback=True \n",
    "exp_manager.checkpoint_callback_params.monitor=\"validation_loss\" \n",
    "exp_manager.checkpoint_callback_params.save_best_model=False \n",
    "exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68c4e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "OmegaConf.save(config, \"llama-config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0017133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv llama-config.yaml /opt/NeMo/examples/nlp/language_modeling/tuning/conf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540792b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_sft.py --config-name=llama-config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c654ee2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
